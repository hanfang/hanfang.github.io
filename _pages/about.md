---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Han Fang is a Research Lead in Core LLM at Meta's AI Frontier org. In 2023-2024, He launched Llama 2 & 3 on Meta AI into Meta's family of apps, growing it to 1B monthly active users. In 2025, he focuses on teaching models to think and reason (becoming agents) and more broadly post training.
​

Han holds a PhD in Applied Mathematics and has published in top-tier venues with 7500+ citations. He is a recipient of the President’s Award to Distinguished Doctoral Students, the Woo-Jong Kim Dissertation Award, and the Excellence in Research Award.

[Google Scholar](https://scholar.google.com/citations?user=mQIqIVwAAAAJ) / CV / [Linkedin](https://www.linkedin.com/in/hfang15/) / [Twitter](https://x.com/Han_Fang_)


News
======
* **Launched [Llama 3 on Meta AI](https://ai.meta.com/blog/meta-llama-3/) and subsequently Llama 3.1**.  
  [Meta AI Blog Post](https://ai.meta.com/blog/meta-llama-3-1/)

* **Launched [Llama 2](https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/) on [Meta AI](https://www.meta.ai/)**.  
  [My talk at Meta's Connect Conference](https://developers.facebook.com/videos/2023/building-metas-next-generation-ai-product-experiences-with-llama/) in 2023


Recent Papers
======
* **Boosting LLM Reasoning via Spontaneous Self-Correction**
  *Xutong Zhao, Tengyu Xu, Xuewei Wang, Zhengxing Chen, Di Jin, Liang Tan, Zishun Yu, Zhuokai Zhao, Yun He, Sinong Wang, Han Fang, Sarath Chandar, Chen Zhu* · [arXiv](https://arxiv.org/abs/2506.06923) (2025)

* **Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation**  
  *Chengwei Qin, Wenxuan Zhou, Karthik Abinav Sankararaman, Nanshu Wang, Tengyu Xu, Alexander Radovic, Eryk Helenowski, Arya Talebzadeh, Aditya Tayade, Sinong Wang, Shafiq Joty, **Han Fang**, Hao Ma* · [**ACL**](https://arxiv.org/abs/2505.12265) (2025)

* **Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization**  
  *Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, Hao Ma, **Han Fang*** · [**ICML**](https://arxiv.org/abs/2501.17974) 2025 

* **Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback**
  *Yen-Ting Lin, Di Jin, Tengyu Xu, Tianhao Wu, Sainbayar Sukhbaatar, Chen Zhu, Yun He, Yun-Nung Chen, Jason Weston, Yuandong Tian, Arash Rahnama, Sinong Wang, Hao Ma, **Han Fang*** · [arXiv](https://arxiv.org/abs/2501.10799) (2025)

* **Improving Model Factuality with Fine-grained Critique-based Evaluator**  
  *Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, **Han Fang**, Carolyn Rose, Daniel Fried, Hejia Zhang* · [**ACL**](https://arxiv.org/abs/2410.18359) 2025

* **Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following**  
  *Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, **Han Fang**, Sinong Wang* · [arXiv](https://arxiv.org/abs/2410.15553) 2024

* **The Perfect Blend: Redefining RLHF with mixture of judges**  
  *Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, Zhouhao Zeng, Yun He, Karishma Mandyam, Arya Talabzadeh, Madian Khabsa, Gabriel Cohen, Yuandong Tian, Hao Ma, Sinong Wang, **Han Fang*** · [arXiv](https://arxiv.org/abs/2409.20370) 2024
